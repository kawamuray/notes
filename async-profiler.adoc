== async-profiler internal

How does it work? From `profiler.sh PID` to write the output.


=== JVMTI entrypoint

It is a JVMTI agent, so the entrypoint should be JVMTI agent's entry point.

[source,java]
----
// vmEntry.cpp
extern "C" JNIEXPORT jint JNICALL
Agent_OnAttach(JavaVM* vm, char* options, void* reserved) {
...
    Profiler::_instance.run(args);
----

1. Profiler has singleton instance
2. Call `Profiler::run`

=== Profiler::run

[source,java]
----
void Profiler::run(Arguments& args) {
...
    } else {
        std::ofstream out(args._file, std::ios::out | std::ios::trunc);
        if (out.is_open()) {
            runInternal(args, out);
            out.close();
...
}
----

1. Open output stream (`-f` option)
2. Call `runInternal`


=== Profiler::runInternal => Profiler::start

[source,java]
----
Error Profiler::start(Arguments& args, bool reset) {
...
    error = _engine->start(args);
...
----

1. Call `Engine::start`

The `_engine` here depends on event type (`-e` option).
Determined by `Profiler::selectEngine`.

[source,java]
----
Engine* Profiler::selectEngine(const char* event_name) {
    if (strcmp(event_name, EVENT_CPU) == 0) {
        return PerfEvents::supported() ? (Engine*)&perf_events : (Engine*)&wall_clock;
    } else if (strcmp(event_name, EVENT_ALLOC) == 0) {
        return &alloc_tracer;
    } else if (strcmp(event_name, EVENT_LOCK) == 0) {
        return &lock_tracer;
    } else if (strcmp(event_name, EVENT_WALL) == 0) {
        return &wall_clock;
    } else if (strcmp(event_name, EVENT_ITIMER) == 0) {
        return &itimer;
    } else if (strchr(event_name, '.') != NULL) {
        return &instrument;
    } else {
        return &perf_events;
    }
}
----

So by default (cpu), its `PerfEvents`.

=== PerfEvents::start

[source,java]
----
Error PerfEvents::start(Arguments& args) {
...
    OS::installSignalHandler(SIGPROF, signalHandler);

...
    // Create perf_events for all existing threads
    bool created = false;
    ThreadList* thread_list = OS::listThreads();
    for (int tid; (tid = thread_list->next()) != -1; ) {
        created |= createForThread(tid);
    }
...
}
----

1. Call `OS::installSignalHandler` with `SIGPROF`
2. Call `PerfEvents::createForThread`.


=== OS::installSignalHandler

[source,java]
----
void OS::installSignalHandler(int signo, SigAction action, SigHandler handler) {
...
    } else {
        sa.sa_sigaction = action;
        sa.sa_flags = SA_SIGINFO | SA_RESTART;
    }

    sigaction(signo, &sa, NULL);
}
----

1. Setup signal by `sigaction`.

Interestingly, `SIGVTALRM` is used for wallclock events:

[source,java]
----
// wallClock.cpp
Error WallClock::start(Arguments& args) {
...
    OS::installSignalHandler(SIGVTALRM, signalHandler);
    OS::installSignalHandler(WAKEUP_SIGNAL, NULL, wakeupHandler);
----

wallclock engine creates a separate thread that simply loops with `sleep()` and send the signal to the target thread by `tgkill(2)`.

=== PerfEvents::createForThread

[source,java]
----
bool PerfEvents::createForThread(int tid) {
...
    int fd = syscall(__NR_perf_event_open, &attr, tid, -1, -1, 0);
...

    struct f_owner_ex ex;
    ex.type = F_OWNER_TID;
    ex.pid = tid;

    fcntl(fd, F_SETFL, O_ASYNC);
    fcntl(fd, F_SETSIG, SIGPROF);
    fcntl(fd, F_SETOWN_EX, &ex);
...
----

1. Open perf event stream.
2. Setup SIGPROF as the signal number for perf events.


=== PerfEvents::signalHandler

A task (thread) signaled will run this handler as it has given to `sigaction(2)`.

[source,java]
----
void PerfEvents::signalHandler(int signo, siginfo_t* siginfo, void* ucontext) {
...
    Profiler::_instance.recordSample(ucontext, counter, 0, NULL);
----

=== Profiler::recordSample

[source,java]
----
void Profiler::recordSample(void* ucontext, u64 counter, jint event_type, jmethodID event, ThreadState thread_state) {
...
    ASGCT_CallFrame* frames = _calltrace_buffer[lock_index]->_asgct_frames;
...
    if (_cstack != CSTACK_NO) {
        num_frames += getNativeTrace(ucontext, frames + num_frames, tid);
    }
...
    } else {
        num_frames += getJavaTraceAsync(ucontext, frames + num_frames, _max_stack_depth);
    }
...
    int call_trace_id = storeCallTrace(num_frames, frames, counter);
    _jfr.recordExecutionSample(lock_index, tid, call_trace_id, thread_state);
----

1. Call `Profiler::getNativeTrace` => `Engine::getNativeTrace` (several impls available; `PerfEvents` uses register info from events, impl in `engine.hpp` (fallback?) implements its using `rbp` and `rip` registers walking.
2. Call `Profiler::getJavaTraceAsync` - main part taking Java method's stack trace.
3. Call `Profiler::storeCallTrace`.

As you can see, both of `getNativeTrace` and `getJavaTraceAsync`(= `AsyncGetCallTrace`) requires `ucontext_t` as an argument.
`ucontext_t` holds the value of registers, which of the task that has been interrupted (which is also the current task executing the signal handler), before it switched the context over to invoke the signal handler. So by its register values, it is possible to walk through the stack frames from top to down, making it possible to collect stack traces. The `ucontext_t` is given as the 3rd argument of the signal handler, when we set `SA_SIGINFO` flag.  link:./perf.adoc[perf] contains a section about how to implement a simple stack walking using this technique.

=== Profiler::getJavaTraceAsync


[source,java]
----
int Profiler::getJavaTraceAsync(void* ucontext, ASGCT_CallFrame* frames, int max_depth) {
...
    ASGCT_CallTrace trace = {jni, 0, frames};
    VM::_asyncGetCallTrace(&trace, max_depth, ucontext);
----

1.  Call `VM::_asyncGetCallTrace`.

=== VM::_asyncGetCallTrace

It's dynamically linked `AsyncGetCallTrace`.

[source,java]
----
void VM::init(JavaVM* vm, bool attach) {
...
    _libjvm = getLibraryHandle("libjvm.so");
    _asyncGetCallTrace = (AsyncGetCallTrace)dlsym(_libjvm, "AsyncGetCallTrace");
    _getManagement = (JVM_GetManagement)dlsym(_libjvm, "JVM_GetManagement");
----


=== Profiler::storeCallTrace

[source,java]
----
int Profiler::storeCallTrace(int num_frames, ASGCT_CallFrame* frames, u64 counter) {
...
    while (_hashes[i] != hash) {
        if (_hashes[i] == 0) {
            if (__sync_bool_compare_and_swap(&_hashes[i], 0, hash)) {
                copyToFrameBuffer(num_frames, frames, &_traces[i]);
                break;
            }
}
----

1. Store trace in `_traces[i]`.

=== Profiler::runInternal(ACTION_DUMP)

At the time to stop profiling (either by `./profiler.sh stop` or after configured duration), `Profiler::runInternal` is called again but with `ACTION_DUMP` argument.

[source,java]
----
void Profiler::runInternal(Arguments& args, std::ostream& out) {
    switch (args._action) {
...
        case ACTION_DUMP:
            stop();
            clearOutput();
            switch (args._output) {
                case OUTPUT_COLLAPSED:
                    dumpCollapsed(out, args);
                    break;
                case OUTPUT_FLAMEGRAPH:
                    dumpFlameGraph(out, args, false);
                    break;
                case OUTPUT_TREE:
                    dumpFlameGraph(out, args, true);
                    break;
                case OUTPUT_TEXT:
                    dumpSummary(out);
                    if (args._dump_traces > 0) dumpTraces(out, args);
                    if (args._dump_flat > 0) dumpFlat(out, args);
                    break;
                default:
                    break;
            }
            break;
}
----

1. Depending on the configured output, it calls the function to write the output.
2. Output function takes traces from `_traces` field of the `Profiler` instance and prints it out.


== JVM crash by assertion error: `guarantee(_nParked == 0) failed: invariant`

Context: I was extending async-profiler's JVMTI to enable realtime stack capture streaming through external SIGPROF.

Crash report:
----
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  Internal Error (os_linux.cpp:6082), pid=135346, tid=0x00007f11281fd700
#  guarantee(_nParked == 0) failed: invariant
#
# JRE version: OpenJDK Runtime Environment (8.0_262-b10) (build 1.8.0_262-b10)
# Java VM: OpenJDK 64-Bit Server VM (25.262-b10 mixed mode linux-amd64 compressed oops)
...
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#

---------------  T H R E A D  ---------------

Current thread (0x00007f1150017000):  JavaThread "kafka-scheduler-8" daemon [_thread_blocked, id=135686, stack(0x00007f11280fd000,0x00007f11281fe000)]

Stack: [0x00007f11280fd000,0x00007f11281fe000],  sp=0x00007f11281f9530,  free space=1009k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.so+0xb5718d]  VMError::report_and_die()+0x15d
V  [libjvm.so+0x506615]  report_vm_error(char const*, int, char const*, char const*)+0xa5
V  [libjvm.so+0x942bd3]  os::PlatformEvent::park()+0x153
V  [libjvm.so+0x8eeff8]  Monitor::ILock(Thread*)+0x248
V  [libjvm.so+0x8ef946]  Monitor::lock_without_safepoint_check()+0x26
V  [libjvm.so+0x9e01a6]  SafepointSynchronize::block(JavaThread*) [clone .part.140]+0x86
V  [libjvm.so+0xaf9c48]  JavaThread::check_safepoint_and_suspend_for_native_trans(JavaThread*)+0x258
V  [libjvm.so+0x74fae7]  jvmti_GetMethodName+0x117
C  [libasyncProfiler.so+0x3059c]  FrameName::javaMethodName(_jmethodID*)+0x4c
C  [libasyncProfiler.so+0x30d55]  FrameName::name(ASGCT_CallFrame&, bool)+0x275
C  [libasyncProfiler.so+0x19e7a]  Profiler::dumpJsonEvent(int, int, CallTraceSample&, FrameName&)+0x32a
C  [libasyncProfiler.so+0x1ade0]  Profiler::recordSample(void*, unsigned long long, int, _jmethodID*, ThreadState)+0x310
C  [libpthread.so.0+0xf5f0]
V  [libjvm.so+0x8eeff8]  Monitor::ILock(Thread*)+0x248
V  [libjvm.so+0x8ef946]  Monitor::lock_without_safepoint_check()+0x26
V  [libjvm.so+0x9e01a6]  SafepointSynchronize::block(JavaThread*) [clone .part.140]+0x86
V  [libjvm.so+0xaf9c48]  JavaThread::check_safepoint_and_suspend_for_native_trans(JavaThread*)+0x258
V  [libjvm.so+0xafa401]  JavaThread::check_special_condition_for_native_trans(JavaThread*)+0x11
J 9441  java.io.FileDescriptor.sync()V (0 bytes) @ 0x00007f132e58d7bf [0x00007f132e58d6c0+0xff]
J 9449 C1 kafka.server.checkpoints.CheckpointFile.liftedTree1$1(Lscala/collection/Iterable;)V (194 bytes) @ 0x00007f132e61d2fc [0x00007f132e61b720+0x1bdc]
J 9447 C1 kafka.server.checkpoints.OffsetCheckpointFile.write(Lscala/collection/Map;)V (9 bytes) @ 0x00007f132d708dcc [0x00007f132d708be0+0x1ec]
j  kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$2(Lkafka/log/LogManager;Lscala/collection/Map;Ljava/io/File;Lkafka/server/checkpoints/OffsetCheckpointFile;)V+28
j  kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$2$adapted(Lkafka/log/LogManager;Lscala/collection/Map;Ljava/io/File;Lkafka/server/checkpoints/OffsetCheckpointFile;)Ljava/lang/Object;+4
j  kafka.log.LogManager$$Lambda$1557.apply(Ljava/lang/Object;)Ljava/lang/Object;+16
J 6968 C2 scala.Option.foreach(Lscala/Function1;)V (19 bytes) @ 0x00007f132e420984 [0x00007f132e420920+0x64]
j  kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$1(Lkafka/log/LogManager;Ljava/io/File;Lscala/collection/Map;)V+18
j  kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$1$adapted(Lkafka/log/LogManager;Ljava/io/File;Lscala/collection/Map;)Ljava/lang/Object;+3
j  kafka.log.LogManager$$Lambda$1553.apply(Ljava/lang/Object;)Ljava/lang/Object;+12
J 6968 C2 scala.Option.foreach(Lscala/Function1;)V (19 bytes) @ 0x00007f132e420984 [0x00007f132e420920+0x64]
j  kafka.log.LogManager.checkpointLogStartOffsetsInDir(Ljava/io/File;)V+20
j  kafka.log.LogManager.$anonfun$checkpointLogStartOffsets$1(Lkafka/log/LogManager;Ljava/io/File;)V+2
j  kafka.log.LogManager.$anonfun$checkpointLogStartOffsets$1$adapted(Lkafka/log/LogManager;Ljava/io/File;)Ljava/lang/Object;+2
j  kafka.log.LogManager$$Lambda$1547.apply(Ljava/lang/Object;)Ljava/lang/Object;+8
J 7945 C2 scala.collection.mutable.ArrayBuffer.foreach(Lscala/Function1;)V (6 bytes) @ 0x00007f132e7ad5e8 [0x00007f132e7ad560+0x88]
j  kafka.log.LogManager.checkpointLogStartOffsets()V+10
j  kafka.log.LogManager.$anonfun$startup$6(Lkafka/log/LogManager;)V+1
j  kafka.log.LogManager$$Lambda$805.apply$mcV$sp()V+4
J 8816 C1 kafka.utils.KafkaScheduler.$anonfun$schedule$2(Lkafka/utils/KafkaScheduler;Ljava/lang/String;Lscala/Function0;)V (65 bytes) @ 0x00007f132dd11eac [0x00007f132dd11960+0x54c]
J 8815 C1 kafka.utils.KafkaScheduler$$Lambda$749.apply$mcV$sp()V (16 bytes) @ 0x00007f132e360fd4 [0x00007f132e360f40+0x94]
J 7856 C1 kafka.utils.CoreUtils$$anon$1.run()V (10 bytes) @ 0x00007f132d595b4c [0x00007f132d595a40+0x10c]
J 9002 C2 java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run()V (59 bytes) @ 0x00007f132e470dc0 [0x00007f132e4709a0+0x420]
j  java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V+95
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub
V  [libjvm.so+0x69c03e]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0xf5e
V  [libjvm.so+0x699404]  JavaCalls::call_virtual(JavaValue*, KlassHandle, Symbol*, Symbol*, JavaCallArguments*, Thread*)+0x2c4
V  [libjvm.so+0x699a19]  JavaCalls::call_virtual(JavaValue*, Handle, KlassHandle, Symbol*, Symbol*, Thread*)+0x59
V  [libjvm.so+0x730ab1]  thread_entry(JavaThread*, Thread*)+0xa1
V  [libjvm.so+0xafe902]  JavaThread::thread_main_inner()+0x212
V  [libjvm.so+0x93a382]  java_start(Thread*)+0xf2
C  [libpthread.so.0+0x7e65]
----


The following is the particular stack caused crash (began by signal handler):

----
V  [libjvm.so+0xb5718d]  VMError::report_and_die()+0x15d
V  [libjvm.so+0x506615]  report_vm_error(char const*, int, char const*, char const*)+0xa5
V  [libjvm.so+0x942bd3]  os::PlatformEvent::park()+0x153
V  [libjvm.so+0x8eeff8]  Monitor::ILock(Thread*)+0x248
V  [libjvm.so+0x8ef946]  Monitor::lock_without_safepoint_check()+0x26
V  [libjvm.so+0x9e01a6]  SafepointSynchronize::block(JavaThread*) [clone .part.140]+0x86
V  [libjvm.so+0xaf9c48]  JavaThread::check_safepoint_and_suspend_for_native_trans(JavaThread*)+0x258
V  [libjvm.so+0x74fae7]  jvmti_GetMethodName+0x117
C  [libasyncProfiler.so+0x3059c]  FrameName::javaMethodName(_jmethodID*)+0x4c
C  [libasyncProfiler.so+0x30d55]  FrameName::name(ASGCT_CallFrame&, bool)+0x275
C  [libasyncProfiler.so+0x19e7a]  Profiler::dumpJsonEvent(int, int, CallTraceSample&, FrameName&)+0x32a
C  [libasyncProfiler.so+0x1ade0]  Profiler::recordSample(void*, unsigned long long, int, _jmethodID*, ThreadState)+0x310
C  [libpthread.so.0+0xf5f0]
----

JVMTI's `GetMethodName` call.

In `JvmtiEnv::GetMethodName`'s body I couldn't see where is it calling `JavaThread::check-safepoint_and_suspend_for_native_trans`, but it is clearly called by backtrace.

[source,cpp]
----
jvmtiError
JvmtiEnv::GetMethodName(Method* method_oop, char** name_ptr, char** signature_ptr, char** generic_ptr) {
  NULL_CHECK(method_oop, JVMTI_ERROR_INVALID_METHODID);
  JavaThread* current_thread  = JavaThread::current();

  ResourceMark rm(current_thread); // get the utf8 name and signature
  if (name_ptr == NULL) {
    // just don't return the name
  } else {
    const char* utf8_name = (const char *) method_oop->name()->as_utf8();
    *name_ptr = (char *) jvmtiMalloc(strlen(utf8_name)+1);
    strcpy(*name_ptr, utf8_name);
  }
  if (signature_ptr == NULL) {
    // just don't return the signature
  } else {
    const char* utf8_signature = (const char *) method_oop->signature()->as_utf8();
    *signature_ptr = (char *) jvmtiMalloc(strlen(utf8_signature) + 1);
    strcpy(*signature_ptr, utf8_signature);
  }

  if (generic_ptr != NULL) {
    *generic_ptr = NULL;
    Symbol* soop = method_oop->generic_signature();
    if (soop != NULL) {
      const char* gen_sig = soop->as_C_string();
      if (gen_sig != NULL) {
        jvmtiError err = allocate(strlen(gen_sig) + 1, (unsigned char **)generic_ptr);
        if (err != JVMTI_ERROR_NONE) {
          return err;
        }
        strcpy(*generic_ptr, gen_sig);
      }
    }
  }
  return JVMTI_ERROR_NONE;
} /* end GetMethodName */
----

Climbing down the stack, I could confirm the `os::Platform::park` method at `os_linux.cpp` indeed has that assertion:

[source,cpp]
----
void os::PlatformEvent::park() {       // AKA "down()"
  // Invariant: Only the thread associated with the Event/PlatformEvent
  // may call park().
  // TODO: assert that _Assoc != NULL or _Assoc == Self
  int v ;
  for (;;) {
      v = _Event ;
      if (Atomic::cmpxchg (v-1, &_Event, v) == v) break ;
  }
  guarantee (v >= 0, "invariant") ;
  if (v == 0) {
     // Do this the hard way by blocking ...
     int status = pthread_mutex_lock(_mutex);
     assert_status(status == 0, status, "mutex_lock");
     guarantee (_nParked == 0, "invariant") ;
     ++ _nParked ;
     while (_Event < 0) {
        status = pthread_cond_wait(_cond, _mutex);
        // for some reason, under 2.7 lwp_cond_wait() may return ETIME ...
        // Treat this the same as if the wait was interrupted
        if (status == ETIME) { status = EINTR; }
        assert_status(status == 0 || status == EINTR, status, "cond_wait");
     }
     -- _nParked ;

    _Event = 0 ;
     status = pthread_mutex_unlock(_mutex);
     assert_status(status == 0, status, "mutex_unlock");
    // Paranoia to ensure our locked and lock-free paths interact
    // correctly with each other.
    OrderAccess::fence();
  }
  guarantee (_Event >= 0, "invariant") ;
}
----

Apparently `_nParked` field keeps track on the entrance state to "park". According the the comment, `Invariant: Only the thread associated with the Event/PlatformEvent may call park()`,  it seems that PlatformEvent has associated Thread that can call `park()`. The type of `_Assoc` is `Thread *`.
I couldn't get immediately the meaning of atomically decrementing the value of `_Event` field but looks like it's not related.

After reading some code around, I couldn't imagine this part becomes an issue. I thought it shouldn't be the bug of this implementation since async-profiler's using `GetMethodName` a lot of times but has never caused JVM crash.

Then I came up with one possibility that could cause this, namely, when the signal interrupted thread was already in this position, then the signal handler will attempt to re-enter this in it's call path, while the `_nParked` is still set to 1 by the original call stack (before interruption).

This is likely the case because I see nearly the same stack trace at just below the stack pushed on by signal handler:

----
C  [libasyncProfiler.so+0x1ade0]  Profiler::recordSample(void*, unsigned long long, int, _jmethodID*, ThreadState)+0x310
C  [libpthread.so.0+0xf5f0]
V  [libjvm.so+0x8eeff8]  Monitor::ILock(Thread*)+0x248
V  [libjvm.so+0x8ef946]  Monitor::lock_without_safepoint_check()+0x26
V  [libjvm.so+0x9e01a6]  SafepointSynchronize::block(JavaThread*) [clone .part.140]+0x86
V  [libjvm.so+0xaf9c48]  JavaThread::check_safepoint_and_suspend_for_native_trans(JavaThread*)+0x258
V  [libjvm.so+0xafa401]  JavaThread::check_special_condition_for_native_trans(JavaThread*)+0x11
----

So the problem is that calling `GetMethodName` from inside the signal handler, which is executed by the original thread (task).


I patched async-profiler to create a separate thread for resolving frame's signature and write it out to a file.
Besides the call to `pthread_create`, the first main routine for the thread looked like this:


[source,cpp]
----
void Profiler::eventWriterLoop() {
    TraceEvent ev = { 0, nullptr };
    while (true) {
        {
            std::unique_lock<std::mutex> lck(_trace_events_lock);
            _trace_events_cv.wait(lck, [this]{ return !_trace_events.empty(); });
            if (!_trace_events.empty()) {
                ev = _trace_events.front();
                _trace_events.pop();
            }
        }
        if (ev.tid) {
            dumpJsonEvent(_out_fd, ev.tid, *ev.trace);
            ev = { 0, nullptr };
        }
    }
}
----


Then the output became like this:

----
\=== 2021-02-05 17:00:21.674 PID: 92640, TID: 92758 (kafka-scheduler), DURATION: 12708686 us
Native Stack:
  0: [0xffffffff894d4fb1] finish_task_switch
  1: [0xffffffff89b80a09] schedule
  2: [0xffffffff89512076] futex_wait_queue_me
...
\--------------------------------------------------------------------------------
JVM Stack (took: 2021-02-05 17:00:21.674):
  0: [0x7f7cf063e7b0] [jvmtiError 115]
  1: [0x7f7cf06b5bb8] [jvmtiError 115]
  2: [0x7f7cf0646fa0] [jvmtiError 115]
  3: [0x7f7cf0595bb0] [jvmtiError 115]
  4: [0x7f7cf06b42a8] [jvmtiError 115]
...
----


According to the https://docs.oracle.com/javase/8/docs/platform/jvmti/jvmti.html#ErrorSection[Error Code in doc], `115` is `JVMTI_ERROR_UNATTACHED_THREAD (115)`.
I've actually knew it by experience at development `wasmtime-java`. In order to call a JNI method (and this time I learn that JVMTI too), we must attach the thread to JVM first. https://github.com/kawamuray/wasmtime-java/blob/791b270e764348116e4cfe101d7105ccd944b738/wasmtime-jni/src/io_github_kawamuray_wasmtime_Func/imp.rs#L123


So I modified the patch to be like:

[source,cpp]
----
void Profiler::eventWriterLoop() {
    // Need to attach this thread to JVM to make a call for JVMTI methods
    JavaVM* jvm;
    VM::jni()->GetJavaVM(&jvm);
    void *env_ptr;
    if (jvm->AttachCurrentThread(&env_ptr, NULL) != JNI_OK) {
        std::cerr << "Failed to attach event writer thread to JVM" << std::endl;
    }

    TraceEvent ev = { 0, nullptr };
...
----

but this ended up with JVM crash, caused by SEGV at this function.
After some consideration I turns out that in order to call `VM::jni()->GetJavaVM(&jvm);`, which is also an JNI function, the thread must be attached.
So this doesn't work, but fortunately `VM` class of async-profiler already held a `JavaVM` instance so I simply exposed it and used instead.


[source,cpp]
----
void Profiler::eventWriterLoop() {
    // Need to attach this thread to JVM to make a call for JVMTI methods
    void *env_ptr;
    if (VM::vm()->AttachCurrentThread(&env_ptr, NULL) != JNI_OK) {
        std::cerr << "Failed to attach event writer thread to JVM" << std::endl;
    }
...
}
----


Then it worked perfectly well. No crash so far.

----
\=== 2021-02-05 19:17:13.74 PID: 146563, TID: 146686 (kafka-scheduler), DURATION: 10000085 us
Native Stack:
  0: [0xffffffff894d4fb1] finish_task_switch
  1: [0xffffffff89b80a09] schedule
  2: [0xffffffff89512076] futex_wait_queue_me
  3: [0xffffffff89512e1b] futex_wait
  4: [0xffffffff89514b66] do_futex
  5: [0xffffffff89515080] sys_futex
  6: [0xffffffff89b8dede] system_call_fastpath
  7: [0x7f76952109f5] [unknown]
  8: [0x7f769429c08e] [unknown]
  9: [0x7f767d8a4d2a] [unknown]
  10: [0x7f767ea1a52c] [unknown]
  11: [0xb9134315b912904c] [unknown]
\--------------------------------------------------------------------------------
JVM Stack (took: 2021-02-05 19:17:13.74):
  0: [0x7f7454564810] pthread_cond_timedwait@@GLIBC_2.3.2
  1: [0x7f74542b4030] Unsafe_Park
  2: [0x7f74547662a0] sun.misc.Unsafe.park
  3: [0x7f74546fae10] java.util.concurrent.locks.LockSupport.parkNanos
  4: [0x7f7454720b40] java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos
  5: [0x7f745471f350] java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take
  6: [0x7f745471f348] java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take
  7: [0x7f74547219a8] java.util.concurrent.ThreadPoolExecutor.getTask
  8: [0x7f7454721910] java.util.concurrent.ThreadPoolExecutor.runWorker
  9: [0x7f74546fae90] java.util.concurrent.ThreadPoolExecutor$Worker.run
  10: [0x7f745476c9b0] java.lang.Thread.run
----
